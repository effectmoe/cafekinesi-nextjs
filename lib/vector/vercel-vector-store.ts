import { config } from 'dotenv';
import { sql } from '@vercel/postgres';
import { createHash } from 'crypto';
import { pipeline } from '@xenova/transformers';

// .env.localãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«èª­ã¿è¾¼ã¿
config({ path: '.env.local' });

export class VercelVectorStore {
  private embedder: any;

  async initialize() {
    // è»½é‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    this.embedder = await pipeline(
      'feature-extraction',
      'Xenova/all-MiniLM-L6-v2'
    );
    console.log('âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†');
  }

  // ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
  private hashContent(content: string): string {
    return createHash('sha256').update(content).digest('hex');
  }

  // ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
  async addDocuments(documents: any[]) {
    console.log(`ğŸ“ ${documents.length}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ä¸­...`);

    for (const doc of documents) {
      try {
        // åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        const output = await this.embedder(doc.content, {
          pooling: 'mean',
          normalize: true
        });
        const embedding = Array.from(output.data);

        // ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        const contentHash = this.hashContent(doc.content);

        // UPSERTå‡¦ç†
        await sql`
          INSERT INTO embeddings (
            content,
            metadata,
            embedding,
            source,
            content_hash,
            search_text
          ) VALUES (
            ${doc.content},
            ${JSON.stringify(doc.metadata || {})},
            ${JSON.stringify(embedding)},
            ${doc.source || 'sanity'},
            ${contentHash},
            to_tsvector('simple', ${doc.content})
          )
          ON CONFLICT (content_hash)
          DO UPDATE SET
            embedding = EXCLUDED.embedding,
            metadata = EXCLUDED.metadata,
            updated_at = NOW()
        `;

      } catch (error) {
        console.error('âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ã‚¨ãƒ©ãƒ¼:', error);
      }
    }

    console.log('âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ å®Œäº†');
  }

  // ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
  async hybridSearch(query: string, options = {}) {
    const { topK = 5, threshold = 0.7 } = options as any;

    // ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    const output = await this.embedder(query, {
      pooling: 'mean',
      normalize: true
    });
    const queryEmbedding = Array.from(output.data);

    // ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ
    const results = await sql`
      WITH vector_search AS (
        SELECT
          id,
          content,
          metadata,
          1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) as vector_score
        FROM embeddings
        WHERE 1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) > ${threshold}
        ORDER BY vector_score DESC
        LIMIT ${topK}
      ),
      text_search AS (
        SELECT
          id,
          ts_rank(search_text, plainto_tsquery('simple', ${query})) as text_score
        FROM embeddings
        WHERE search_text @@ plainto_tsquery('simple', ${query})
      )
      SELECT
        v.content,
        v.metadata,
        v.vector_score,
        COALESCE(t.text_score, 0) as text_score,
        (v.vector_score * 0.7 + COALESCE(t.text_score, 0) * 0.3) as combined_score
      FROM vector_search v
      LEFT JOIN text_search t ON v.id = t.id
      ORDER BY combined_score DESC
    `;

    return results.rows;
  }

  // çµ±è¨ˆæƒ…å ±å–å¾—
  async getStats() {
    const stats = await sql`
      SELECT
        COUNT(*) as total_documents,
        COUNT(DISTINCT source) as sources,
        MAX(updated_at) as last_update
      FROM embeddings
    `;

    return stats.rows[0];
  }
}